{
  "artifacts": [],
  "command": "main",
  "experiment": {
    "base_dir": "/Users/karina/Documents/Coding/PyCharm/mme/src",
    "dependencies": [
      "numpy==1.26.4",
      "sacred==0.8.5"
    ],
    "mainfile": "experiment.py",
    "name": "tinypaper",
    "repositories": [
      {
        "commit": "863fe55c40d6aca96fb4a40bcbed10ae02e51cd2",
        "dirty": false,
        "url": "https://github.com/fdamken/tinypaper_code.git"
      },
      {
        "commit": "863fe55c40d6aca96fb4a40bcbed10ae02e51cd2",
        "dirty": false,
        "url": "https://github.com/fdamken/tinypaper_code.git"
      }
    ],
    "sources": [
      [
        "experiment.py",
        "_sources/experiment_7ab7c4f9e536be33bd5f71ad8c083f8f.py"
      ],
      [
        "path_util.py",
        "_sources/path_util_3a188cc169f54562aefe4201454e44af.py"
      ]
    ]
  },
  "heartbeat": "2024-05-08T10:48:23.444728",
  "host": {
    "ENV": {},
    "cpu": "Apple M2",
    "hostname": "MacBook-Pro-von-Karina.local",
    "os": [
      "Darwin",
      "macOS-10.16-x86_64-i386-64bit"
    ],
    "python_version": "3.11.9"
  },
  "meta": {
    "command": "main",
    "config_updates": {
      "from_session_id": 200,
      "language": "ar",
      "model_name": "gemini-1.0-pro-001",
      "to_session_id": 250
    },
    "named_configs": [],
    "options": {
      "--beat-interval": null,
      "--capture": null,
      "--comment": null,
      "--debug": false,
      "--enforce_clean": false,
      "--file_storage": null,
      "--force": false,
      "--help": false,
      "--id": null,
      "--loglevel": null,
      "--mongo_db": null,
      "--name": null,
      "--pdb": false,
      "--print-config": false,
      "--priority": null,
      "--queue": false,
      "--s3": null,
      "--sql": null,
      "--tiny_db": null,
      "--unobserved": false,
      "COMMAND": null,
      "UPDATE": [
        "model_name=gemini-1.0-pro-001",
        "language=ar",
        "from_session_id=200",
        "to_session_id=250"
      ],
      "help": false,
      "with": true
    }
  },
  "resources": [],
  "result": {
    "answers": [
      [
        2,
        2,
        1,
        1,
        2,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2
      ],
      [
        2,
        2,
        2,
        1,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        1,
        2
      ],
      [
        2,
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        2,
        1,
        2,
        1,
        1
      ],
      [
        2,
        1,
        2,
        2,
        1,
        2,
        2,
        2,
        2,
        1,
        1,
        1,
        1
      ],
      [
        1,
        1,
        1,
        1,
        1,
        1,
        2,
        1,
        1,
        1,
        1,
        1,
        2
      ],
      [
        2,
        2,
        1,
        2,
        1,
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        2
      ],
      [
        2,
        2,
        1,
        2,
        2,
        2,
        2,
        1,
        1,
        2,
        1,
        1,
        2
      ],
      [
        2,
        1,
        2,
        2,
        2,
        1,
        1,
        1,
        1,
        2,
        1,
        2,
        1
      ],
      [
        1,
        2,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2
      ],
      "Traceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 80, in _fetch\n    response = self._chat.send_message(prompt)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/google/generativeai/generative_models.py\", line 434, in send_message\n    self._check_response(response=response, stream=stream)\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/google/generativeai/generative_models.py\", line 453, in _check_response\n    raise generation_types.BlockedPromptException(response.prompt_feedback)\ngoogle.generativeai.types.generation_types.BlockedPromptException: block_reason: OTHER\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 26, in play\n    answer = self._prompt(self._make_prompt(scenario))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 38, in _prompt\n    result = self._model.prompt(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 35, in prompt\n    return self._fetch(prompt)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 314, in iter\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/sacred/config/captured_function.py\", line 42, in captured_function\n    result = wrapped(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 83, in _fetch\n    raise LogSessionStateDetailsException() from exc\nutil.LogSessionStateDetailsException\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/scripts/experiments/run.py\", line 16, in _run_session\n    result = agent.play(session)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 314, in iter\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 31, in play\n    raise Exception(f\"failed to prompt for scenario {scenario_idx}; answers so for: {answers}\") from exc\nException: failed to prompt for scenario 12; answers so for: [1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2]\n",
      [
        2,
        2,
        1,
        2,
        1,
        1,
        1,
        2,
        1,
        1,
        1,
        1,
        2
      ],
      [
        2,
        1,
        1,
        2,
        2,
        2,
        1,
        2,
        1,
        1,
        1,
        1,
        1
      ],
      [
        1,
        1,
        1,
        1,
        1,
        2,
        1,
        1,
        1,
        1,
        1,
        2,
        1
      ],
      [
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        1
      ],
      [
        1,
        1,
        2,
        2,
        1,
        1,
        1,
        1,
        2,
        2,
        1,
        1,
        1
      ],
      [
        2,
        2,
        1,
        1,
        2,
        2,
        1,
        2,
        2,
        1,
        1,
        2,
        1
      ],
      [
        2,
        1,
        2,
        2,
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        1
      ],
      [
        1,
        2,
        1,
        1,
        2,
        2,
        2,
        1,
        2,
        1,
        2,
        1,
        1
      ],
      [
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        2,
        2,
        1,
        2,
        1,
        1
      ],
      [
        2,
        2,
        1,
        2,
        2,
        1,
        2,
        1,
        1,
        2,
        2,
        2,
        2
      ],
      [
        1,
        1,
        1,
        1,
        2,
        1,
        2,
        1,
        1,
        2,
        2,
        2,
        2
      ],
      [
        1,
        1,
        2,
        2,
        2,
        1,
        1,
        2,
        2,
        2,
        2,
        1,
        2
      ],
      [
        1,
        2,
        1,
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        1,
        2
      ],
      "Traceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 80, in _fetch\n    response = self._chat.send_message(prompt)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/google/generativeai/generative_models.py\", line 434, in send_message\n    self._check_response(response=response, stream=stream)\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/google/generativeai/generative_models.py\", line 453, in _check_response\n    raise generation_types.BlockedPromptException(response.prompt_feedback)\ngoogle.generativeai.types.generation_types.BlockedPromptException: block_reason: OTHER\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 26, in play\n    answer = self._prompt(self._make_prompt(scenario))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 38, in _prompt\n    result = self._model.prompt(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 35, in prompt\n    return self._fetch(prompt)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 314, in iter\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/sacred/config/captured_function.py\", line 42, in captured_function\n    result = wrapped(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 83, in _fetch\n    raise LogSessionStateDetailsException() from exc\nutil.LogSessionStateDetailsException\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/scripts/experiments/run.py\", line 16, in _run_session\n    result = agent.play(session)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 314, in iter\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 31, in play\n    raise Exception(f\"failed to prompt for scenario {scenario_idx}; answers so for: {answers}\") from exc\nException: failed to prompt for scenario 6; answers so for: [2, 1, 2, 1, 1, 2]\n",
      [
        2,
        2,
        2,
        1,
        2,
        2,
        1,
        1,
        1,
        2,
        1,
        1,
        2
      ],
      [
        1,
        1,
        2,
        2,
        2,
        2,
        1,
        2,
        1,
        1,
        1,
        1,
        2
      ],
      [
        2,
        2,
        2,
        2,
        1,
        1,
        2,
        2,
        1,
        1,
        2,
        2,
        1
      ],
      [
        1,
        2,
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        2,
        2,
        1,
        2
      ],
      [
        1,
        1,
        1,
        2,
        1,
        1,
        2,
        1,
        2,
        2,
        1,
        1,
        1
      ],
      [
        2,
        2,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        2,
        2,
        2,
        2
      ],
      "Traceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 80, in _fetch\n    response = self._chat.send_message(prompt)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/google/generativeai/generative_models.py\", line 434, in send_message\n    self._check_response(response=response, stream=stream)\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/google/generativeai/generative_models.py\", line 453, in _check_response\n    raise generation_types.BlockedPromptException(response.prompt_feedback)\ngoogle.generativeai.types.generation_types.BlockedPromptException: block_reason: OTHER\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 26, in play\n    answer = self._prompt(self._make_prompt(scenario))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 38, in _prompt\n    result = self._model.prompt(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 35, in prompt\n    return self._fetch(prompt)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 314, in iter\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/sacred/config/captured_function.py\", line 42, in captured_function\n    result = wrapped(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/model/google.py\", line 83, in _fetch\n    raise LogSessionStateDetailsException() from exc\nutil.LogSessionStateDetailsException\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/scripts/experiments/run.py\", line 16, in _run_session\n    result = agent.play(session)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 314, in iter\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/karina/opt/anaconda3/envs/LLMs/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/karina/Documents/Coding/PyCharm/mme/src/agent.py\", line 31, in play\n    raise Exception(f\"failed to prompt for scenario {scenario_idx}; answers so for: {answers}\") from exc\nException: failed to prompt for scenario 12; answers so for: [1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1]\n",
      [
        1,
        2,
        1,
        1,
        1,
        2,
        2,
        2,
        1,
        2,
        1,
        1,
        1
      ],
      [
        1,
        2,
        2,
        2,
        2,
        2,
        1,
        1,
        2,
        2,
        1,
        1,
        1
      ],
      [
        2,
        2,
        2,
        1,
        1,
        1,
        2,
        1,
        1,
        2,
        1,
        2,
        2
      ],
      [
        1,
        2,
        2,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        1,
        2,
        1
      ],
      [
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        1,
        2
      ],
      [
        2,
        1,
        2,
        2,
        1,
        1,
        1,
        2,
        1,
        1,
        2,
        2,
        1
      ],
      [
        1,
        2,
        1,
        1,
        1,
        1,
        2,
        2,
        2,
        1,
        2,
        2,
        2
      ],
      [
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        1,
        2,
        1,
        1,
        2,
        2
      ],
      [
        2,
        1,
        2,
        2,
        2,
        1,
        1,
        1,
        2,
        2,
        2,
        1,
        2
      ],
      [
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        2,
        1,
        2,
        1,
        2,
        2
      ],
      [
        2,
        2,
        1,
        1,
        1,
        2,
        1,
        1,
        1,
        2,
        1,
        1,
        1
      ],
      [
        2,
        1,
        1,
        2,
        1,
        1,
        1,
        1,
        1,
        1,
        2,
        2,
        1
      ],
      [
        2,
        1,
        2,
        1,
        1,
        1,
        1,
        1,
        2,
        1,
        2,
        1,
        1
      ],
      [
        2,
        1,
        2,
        1,
        2,
        1,
        2,
        1,
        1,
        1,
        1,
        2,
        1
      ],
      [
        1,
        1,
        2,
        2,
        2,
        2,
        1,
        1,
        1,
        2,
        1,
        1,
        1
      ],
      [
        2,
        2,
        1,
        1,
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        2
      ],
      [
        2,
        2,
        1,
        2,
        2,
        2,
        1,
        2,
        2,
        1,
        2,
        1,
        2
      ],
      [
        2,
        2,
        2,
        2,
        2,
        2,
        1,
        1,
        2,
        2,
        2,
        2,
        1
      ],
      [
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        1,
        2,
        2,
        2,
        1
      ]
    ],
    "api_usage": [
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      },
      {
        "cost": 0.0,
        "name": "gemini-1.0-pro-001",
        "num_input_tokens": -1,
        "num_output_tokens": -1,
        "py/object": "api_usage.APIUsage"
      }
    ],
    "api_usage_total": {
      "cost": 0.0,
      "name": "gemini-1.0-pro-001",
      "num_input_tokens": -50,
      "num_output_tokens": -50,
      "py/object": "api_usage.APIUsage"
    }
  },
  "start_time": "2024-05-08T10:21:27.867832",
  "status": "COMPLETED",
  "stop_time": "2024-05-08T10:48:21.426341"
}